{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 5: Regression Analysis\n",
    "\n",
    "This notebook contains solutions for Questions 1, 2, 3, and 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Ridge Regression with Gradient Descent\n",
    "\n",
    "**Objective:** \n",
    "1. Generate a dataset with at least seven highly correlated columns.\n",
    "2. Implement Ridge Regression using Gradient Descent Optimization.\n",
    "3. Test different learning rates (0.0001, 0.001, 0.01, 0.1, 1, 10) with regularization parameter $10^{-5}$.\n",
    "4. Choose the best parameters for minimum cost and maximum R2 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# 1. Generate Data with high correlation (effective_rank < n_features creates correlation)\n",
    "X, y = make_regression(n_samples=500, n_features=7, n_informative=7, effective_rank=2, noise=10, random_state=42)\n",
    "\n",
    "print(\"Generated Data Shape:\", X.shape)\n",
    "\n",
    "# Preprocessing\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Add intercept column (bias)\n",
    "X = np.c_[np.ones(X.shape[0]), X]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Gradient Descent Implementation\n",
    "def ridge_gradient_descent(X, y, learning_rate, lambda_param, iterations=1000):\n",
    "    m, n = X.shape\n",
    "    weights = np.zeros(n)\n",
    "    cost_history = []\n",
    "\n",
    "    for i in range(iterations):\n",
    "        # Prediction\n",
    "        y_pred = X.dot(weights)\n",
    "        \n",
    "        # Error\n",
    "        error = y_pred - y\n",
    "        \n",
    "        # Gradient\n",
    "        gradient = (2/m) * X.T.dot(error) + (2 * lambda_param * weights)\n",
    "        \n",
    "        # Update weights\n",
    "        weights = weights - learning_rate * gradient\n",
    "        \n",
    "        # Cost (MSE + Penalty)\n",
    "        cost = np.mean(error**2) + lambda_param * np.sum(weights**2)\n",
    "        cost_history.append(cost)\n",
    "        \n",
    "    return weights, cost_history\n",
    "\n",
    "# Testing Learning Rates\n",
    "learning_rates = [0.0001, 0.001, 0.01, 0.1, 1, 10]\n",
    "lambda_val = 1e-5\n",
    "\n",
    "best_r2 = -float('inf')\n",
    "best_lr = None\n",
    "best_weights = None\n",
    "\n",
    "print(\"\\nTesting Learning Rates:\")\n",
    "for lr in learning_rates:\n",
    "    weights, costs = ridge_gradient_descent(X_train, y_train, lr, lambda_val)\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred_test = X_test.dot(weights)\n",
    "    score = r2_score(y_test, y_pred_test)\n",
    "    \n",
    "    print(f\"LR: {lr}, Final Cost: {costs[-1]:.4f}, R2 Score: {score:.4f}\")\n",
    "    \n",
    "    if score > best_r2:\n",
    "        best_r2 = score\n",
    "        best_lr = lr\n",
    "        best_weights = weights\n",
    "\n",
    "print(f\"\\nBest Learning Rate: {best_lr} with R2 Score: {best_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Hitters Dataset Analysis\n",
    "\n",
    "**Objective:** Pre-process the Hitters data, perform scaling, and fit Linear, Ridge, and LASSO models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, RidgeCV, LassoCV\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# ---------------------------\n",
    "# DATASET PREVIEW (NUMPY STYLE)\n",
    "# ---------------------------\n",
    "data_hitters = pd.read_csv('Hitters (1).csv')\n",
    "print(\"Hitters Dataset (First 5 rows as numpy array):\\n\")\n",
    "print(data_hitters.head().values)\n",
    "\n",
    "# (a) Pre-process the data\n",
    "# Remove rows with missing values\n",
    "data_hitters = data_hitters.dropna()\n",
    "\n",
    "# Convert categorical columns to numerical using One-Hot Encoding (drop_first to avoid dummy variable trap)\n",
    "data_hitters = pd.get_dummies(data_hitters, drop_first=True)\n",
    "\n",
    "# (b) Separate input (X) and output (y) and perform scaling\n",
    "X_h = data_hitters.drop('Salary', axis=1)\n",
    "y_h = data_hitters['Salary']\n",
    "\n",
    "# Split into training and testing sets (70% train, 30% test)\n",
    "X_train_h, X_test_h, y_train_h, y_test_h = train_test_split(X_h, y_h, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standard Scaling\n",
    "scaler_h = StandardScaler()\n",
    "X_train_h_scaled = scaler_h.fit_transform(X_train_h)\n",
    "X_test_h_scaled = scaler_h.transform(X_test_h)\n",
    "\n",
    "# (c) Fit Linear, Ridge, and LASSO models\n",
    "# Linear Regression\n",
    "lin_reg_h = LinearRegression()\n",
    "lin_reg_h.fit(X_train_h_scaled, y_train_h)\n",
    "\n",
    "# Ridge Regression (alpha = 0.5748)\n",
    "ridge_reg_h = Ridge(alpha=0.5748)\n",
    "ridge_reg_h.fit(X_train_h_scaled, y_train_h)\n",
    "\n",
    "# LASSO Regression (alpha = 0.5748)\n",
    "lasso_reg_h = Lasso(alpha=0.5748)\n",
    "lasso_reg_h.fit(X_train_h_scaled, y_train_h)\n",
    "\n",
    "# (d) Evaluate performance\n",
    "y_pred_lin_h = lin_reg_h.predict(X_test_h_scaled)\n",
    "y_pred_ridge_h = ridge_reg_h.predict(X_test_h_scaled)\n",
    "y_pred_lasso_h = lasso_reg_h.predict(X_test_h_scaled)\n",
    "\n",
    "score_lin_h = r2_score(y_test_h, y_pred_lin_h)\n",
    "score_ridge_h = r2_score(y_test_h, y_pred_ridge_h)\n",
    "score_lasso_h = r2_score(y_test_h, y_pred_lasso_h)\n",
    "\n",
    "print(\"\\n--- Question 2 Results ---\")\n",
    "print(\"Linear Regression R2:\", score_lin_h)\n",
    "print(\"Ridge Regression R2:\", score_ridge_h)\n",
    "print(\"LASSO Regression R2:\", score_lasso_h)\n",
    "\n",
    "print(\"\\nBest Model for Hitters:\")\n",
    "if score_ridge_h > score_lin_h and score_ridge_h > score_lasso_h:\n",
    "    print(\"Ridge performed best.\")\n",
    "elif score_lasso_h > score_lin_h and score_lasso_h > score_ridge_h:\n",
    "    print(\"LASSO performed best.\")\n",
    "else:\n",
    "    print(\"Linear Regression performed best.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: Cross Validation\n",
    "\n",
    "**Objective:** Explore Ridge Cross Validation (RidgeCV) and Lasso Cross Validation (LassoCV) to find the optimal alpha for the Hitters dataset and compare it with the alpha used in Q2 (0.5748)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RidgeCV: Built-in Cross Validation for Ridge\n",
    "# We test a range of alphas\n",
    "alphas_to_test = [0.001, 0.01, 0.1, 0.5748, 1, 10, 100]\n",
    "\n",
    "ridge_cv = RidgeCV(alphas=alphas_to_test, scoring='r2')\n",
    "ridge_cv.fit(X_train_h_scaled, y_train_h)\n",
    "\n",
    "# LassoCV: Built-in Cross Validation for Lasso\n",
    "lasso_cv = LassoCV(alphas=alphas_to_test, cv=5, random_state=42)\n",
    "lasso_cv.fit(X_train_h_scaled, y_train_h)\n",
    "\n",
    "print(\"--- Question 3 Results ---\")\n",
    "print(\"Optimal Alpha found by RidgeCV:\", ridge_cv.alpha_)\n",
    "print(\"Optimal Alpha found by LassoCV:\", lasso_cv.alpha_)\n",
    "print(\"Alpha used in Q2: 0.5748\")\n",
    "\n",
    "print(\"\\nComparison:\")\n",
    "if ridge_cv.alpha_ == 0.5748:\n",
    "    print(\"RidgeCV chose the same alpha as Q2.\")\n",
    "else:\n",
    "    print(\"RidgeCV chose a different alpha, suggesting 0.5748 might not be optimal.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: Boston Housing Dataset Analysis\n",
    "\n",
    "**Objective:** Perform a similar regression analysis on the Boston Housing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Boston Housing Data\n",
    "data_boston = pd.read_csv('Boston_Housing.csv')\n",
    "\n",
    "print(\"Boston Housing Dataset (First 5 rows as numpy array):\\n\")\n",
    "print(data_boston.head().values)\n",
    "\n",
    "# Pre-processing\n",
    "# Check for nulls (usually none, but good practice)\n",
    "data_boston = data_boston.dropna()\n",
    "\n",
    "# Separate Input (X) and Output (y)\n",
    "# 'MEDV' is the median value of owner-occupied homes (Target)\n",
    "X_b = data_boston.drop('MEDV', axis=1)\n",
    "y_b = data_boston['MEDV']\n",
    "\n",
    "# Split Data\n",
    "X_train_b, X_test_b, y_train_b, y_test_b = train_test_split(X_b, y_b, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale Data\n",
    "scaler_b = StandardScaler()\n",
    "X_train_b_scaled = scaler_b.fit_transform(X_train_b)\n",
    "X_test_b_scaled = scaler_b.transform(X_test_b)\n",
    "\n",
    "# Fit Models\n",
    "# Linear\n",
    "lin_reg_b = LinearRegression()\n",
    "lin_reg_b.fit(X_train_b_scaled, y_train_b)\n",
    "\n",
    "# Ridge (Using Q2 alpha for consistency)\n",
    "ridge_reg_b = Ridge(alpha=0.5748)\n",
    "ridge_reg_b.fit(X_train_b_scaled, y_train_b)\n",
    "\n",
    "# Lasso (Using Q2 alpha for consistency)\n",
    "lasso_reg_b = Lasso(alpha=0.5748)\n",
    "lasso_reg_b.fit(X_train_b_scaled, y_train_b)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_lin_b = lin_reg_b.predict(X_test_b_scaled)\n",
    "y_pred_ridge_b = ridge_reg_b.predict(X_test_b_scaled)\n",
    "y_pred_lasso_b = lasso_reg_b.predict(X_test_b_scaled)\n",
    "\n",
    "score_lin_b = r2_score(y_test_b, y_pred_lin_b)\n",
    "score_ridge_b = r2_score(y_test_b, y_pred_ridge_b)\n",
    "score_lasso_b = r2_score(y_test_b, y_pred_lasso_b)\n",
    "\n",
    "print(\"\\n--- Question 4 Results (Boston Housing) ---\")\n",
    "print(\"Linear Regression R2:\", score_lin_b)\n",
    "print(\"Ridge Regression R2:\", score_ridge_b)\n",
    "print(\"LASSO Regression R2:\", score_lasso_b)\n",
    "\n",
    "# Determine Best\n",
    "print(\"\\nBest Model for Boston Housing:\")\n",
    "if score_ridge_b > score_lin_b and score_ridge_b > score_lasso_b:\n",
    "    print(\"Ridge performed best.\")\n",
    "elif score_lasso_b > score_lin_b and score_lasso_b > score_ridge_b:\n",
    "    print(\"LASSO performed best.\")\n",
    "else:\n",
    "    print(\"Linear Regression performed best.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}